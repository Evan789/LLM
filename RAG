RAG 的代码实现流程

1. 数据准备 & 向量化

收集你的文档（PDF、网页、数据库记录等）。

使用 embedding 模型（如 OpenAI Embeddings、Sentence-BERT）把文档切分成片段，并转化为向量。

把这些向量存入一个 向量数据库（如 Pinecone、FAISS、Weaviate）。
------
from openai import OpenAI
import faiss
import numpy as np

client = OpenAI()

# 示例文档
docs = ["宝马发布了最新 recall...", "RAG 是一种增强生成模型的架构..."]

# 获取向量
embeddings = [client.embeddings.create(model="text-embedding-ada-002", input=doc).data[0].embedding for doc in docs]

# 存入向量库 (FAISS)
dim = len(embeddings[0])
index = faiss.IndexFlatL2(dim)
index.add(np.array(embeddings).astype('float32'))
------

2. 用户输入查询

用户提出问题，把问题转化为向量 embedding。

在向量库中检索最相关的文档。

------
query = "宝马 recall 一般涉及哪些更新？"
query_vec = client.embeddings.create(model="text-embedding-ada-002", input=query).data[0].embedding

# 检索前2个最相似文档
D, I = index.search(np.array([query_vec]).astype('float32'), k=2)
retrieved_docs = [docs[i] for i in I[0]]
------


3. 组合上下文 & 调用LLM

把用户问题和检索到的文档拼接在一起，作为 LLM 的输入。
------
context = "\n".join(retrieved_docs)
prompt = f"已知信息：{context}\n\n请回答用户问题：{query}"

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": prompt}]
)

print(response.choices[0].message.content)
------

4. 输出结果

LLM 基于检索到的上下文生成答案。

答案既结合了大模型的语言能力，也利用了外部知识。

🔹 总结

代码实现 RAG 的大致步骤是：

准备数据 → 建立向量库

用户输入 → 转 embedding

向量检索 → 找相关文档

拼接上下文 → 调用LLM生成答案
