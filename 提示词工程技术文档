 1. CoT（Chain of Thought，思维链提示）
✅ 概念
核心思想：在提示词中引导模型逐步推理，而不是直接让它输出答案。
就像你解数学题一样，不会直接写出最终答案，而是 分步骤推理。
这样可以大大提升模型在复杂问题上的推理能力。
✅ 案例
问题：小明有 3 个苹果，他又买了 5 个，吃掉 2 个，还剩多少个？

普通提示：

TEXT
小明现在有多少个苹果？
👉 模型可能会随便输出 6 或者出错。

CoT 提示：

TEXT
请逐步思考并写出推理过程：
小明有 3 个苹果，他又买了 5 个，吃掉 2 个，还剩多少个？
模型输出（示例）：

TEXT
小明最开始有 3 个 → 买了 5 个 → 一共 8 个 → 吃掉 2 个 → 剩下 6 个。
答案是：6。
➡ 优势：通过“逐步思考”，模型能更可靠地推理出正确答案。

-------------------------------------------------------------------------

🔹 2. Few-Shot Prompting（小样本提示）
✅ 概念
核心思想：给模型几个示例，让它学会模仿格式/风格，从而更好地完成任务。
类似于老师先做两道题，再让学生做第三道。
✅ 案例
任务：把英文句子翻译成中文。

Zero-Shot（不给示例）：

TEXT
Translate the following English sentences into Chinese:
1. I love programming.
可能翻译正确，也可能跑偏。

Few-Shot（小样本示例）：

TEXT
Translate the following English sentences into Chinese.
Example 1:
English: I go to school.
Chinese: 我去上学。

Example 2:
English: She likes reading books.
Chinese: 她喜欢读书。

Now translate:
English: I love programming.
模型输出：

TEXT
Chinese: 我爱编程。
➡ 优势：Few-Shot 能帮助模型快速抓住“模式”，输出符合我们要求的结果。

----------------------------------------------------------------------

🔹 3. ReAct（Reason + Act 提示）
✅ 概念
核心思想：结合 推理（Reasoning） 和 行动（Acting）。
模型不仅要“思考”（比如进行逻辑推理），还要“行动”（比如调用工具/API、查资料等），最后再综合行动结果得出答案。
常见于 智能体（Agent）应用。
✅ 案例
任务：问 “法国的首都是哪里？并查一下今天巴黎的天气。”

普通提示：

TEXT
法国的首都是哪里？今天巴黎的天气如何？
👉 结果可能错误，因为 LLM 的知识库不一定有最新天气。

ReAct 提示：

TEXT
请逐步解决问题：
1. 首先推理法国的首都是哪个城市。
2. 然后调用天气查询工具获取该城市的实时天气。
3. 最后结合步骤1和步骤2，给出完整答案。
模型可能输出：

TEXT
推理：法国的首都是巴黎。
行动：调用天气API → 查询 “巴黎今日天气”。
结果：多云，气温 22℃。
最终答案：法国的首都是巴黎，今天巴黎的天气是多云，气温 22℃。
➡ 优势：ReAct 技术可以让模型“既有大脑又有手脚”，具备 推理+执行 的完整能力。

🔹 4. 三者对比总结
技术	核心思想	适用场景	案例
CoT	让模型逐步推理，而不是直接答案	数学题、逻辑题、多步骤任务	算苹果数量
Few-Shot	给几个示例，让模型模仿输入输出模式	翻译、文本分类、风格模仿	翻译英文句子
ReAct	推理 + 行动（用工具查询/执行）	智能助理、结合API动态数据的任务	查巴黎天气
✅ 这三种技术经常可以结合使用：
比如 Few-Shot + CoT（先给几个示例解题步骤，再让模型模仿推理），或者 ReAct + CoT（行动前先分步骤推理）。
