from peft import LoraConfig, get_peft_model
from transformers import AutoModelForSequenceClassification, AutoTokenizer

####
peftï¼šHugging Face çš„å‚æ•°é«˜æ•ˆå¾®è°ƒå·¥å…·åº“ï¼Œé‡Œé¢å®ç°äº† LoRAã€Prefix-Tuningã€Adapter ç­‰ã€‚
LoraConfigï¼šç”¨æ¥å®šä¹‰ LoRA çš„å‚æ•°é…ç½®ï¼Œæ¯”å¦‚ rï¼ˆä½ç§©çŸ©é˜µå¤§å°ï¼‰ã€alphaï¼ˆç¼©æ”¾å› å­ï¼‰ã€‚
get_peft_modelï¼šç”¨ LoRA é…ç½®â€œåŒ…è£…â€åŸå§‹æ¨¡å‹ï¼ŒæŠŠå®ƒå˜æˆå¯ä»¥è®­ç»ƒ LoRA å‚æ•°çš„æ¨¡å‹ã€‚
AutoModelForSequenceClassificationï¼šä»é¢„è®­ç»ƒå¤§æ¨¡å‹ç”Ÿæˆä¸€ä¸ªåˆ†ç±»æ¨¡å‹ã€‚
AutoTokenizerï¼šåˆ†è¯å™¨ã€‚
####
-----------------------------------------------------------------------------------------------------------

Step 1 åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

#####
bert-base-uncasedï¼šé¢„è®­ç»ƒ BERT æ¨¡å‹ï¼ˆå°å†™å­—æ¯ï¼Œæ— åŒºåˆ†å¤§å°å†™ï¼‰ã€‚
AutoTokenizer.from_pretrainedï¼šåŠ è½½å¯¹åº” tokenizerã€‚
AutoModelForSequenceClassification.from_pretrainedï¼šåœ¨ BERT ä¸ŠåŠ åˆ†ç±»ä»»åŠ¡å¤´ï¼ˆåˆ†ç±»å±‚ï¼Œnum_labels=2 è¡¨ç¤ºäºŒåˆ†ç±»ï¼‰ã€‚
#####

------------------------------------------------------------------------------------------------------------

Step 2 å®šä¹‰LoRAé…ç½®

lora_config = LoraConfig(
    task_type="SEQ_CLS",  
    r=8,                  
    lora_alpha=16,        
    lora_dropout=0.05     
)

#####
task_type="SEQ_CLS" â†’ æŒ‡å®šä»»åŠ¡æ˜¯ åºåˆ—åˆ†ç±»ï¼Œå‘Šè¯‰ PEFT æ¡†æ¶è¯¥æ€ä¹ˆæ³¨å…¥ LoRAã€‚
r=8 â†’ LoRA çš„ç§©ï¼ˆrankï¼‰ï¼Œå†³å®šæ–°å¢çŸ©é˜µçš„ç»´åº¦ã€‚æ¯”å¦‚ q/k/v çš„æƒé‡çŸ©é˜µåŸæœ¬æ˜¯ 768Ã—768ï¼Œç°åœ¨æ’å…¥çš„ LoRA åªéœ€è¦ 768Ã—8 + 8Ã—768 çš„å‚æ•°ï¼ˆè¿œå°äº 768Ã—768ï¼‰ã€‚
lora_alpha=16 â†’ ç¼©æ”¾å› å­ï¼Œç±»ä¼¼å­¦ä¹ ç‡è°ƒèŠ‚ï¼Œç”¨äºæ§åˆ¶ LoRA çš„æ›´æ–°å¼ºåº¦ã€‚
lora_dropout=0.05 â†’ åœ¨ LoRA çš„æƒé‡åº”ç”¨ dropoutï¼Œç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜ã€‚
ğŸ‘‰ æ€»ä¹‹ï¼Œè¿™ä¸€æ­¥å°±æ˜¯â€œå®šä¹‰ LoRA è¦æ³¨å…¥ä»€ä¹ˆæ ·çš„å°å‚æ•°ç»“æ„â€ã€‚
#####

-----------------------------------------------------------------------------------------------------------

Step 3 åŒ…è£…æ¨¡å‹ã€‚åŠ ä¸ŠLoRA

lora_model = get_peft_model(model, lora_config)

####
get_peft_modelï¼šæ ¹æ® lora_configï¼Œåœ¨ model çš„ç‰¹å®šå±‚ï¼ˆé€šå¸¸æ˜¯ q_proj, v_proj ç­‰æ³¨æ„åŠ›æƒé‡ï¼‰æ’å…¥ LoRA æ¨¡å—ã€‚
æ›¿æ¢åçš„æ¨¡å‹ï¼š
åŸå§‹ BERT çš„å‚æ•° å…¨éƒ¨è¢«å†»ç»“ã€‚
åªæ–°å¢ä¸€å°éƒ¨åˆ† LoRA å‚æ•°ï¼ˆä½ç§© Aã€B çŸ©é˜µï¼‰éœ€è¦è®­ç»ƒã€‚
è¿™æ ·ï¼Œè®­ç»ƒæ—¶æ˜¾å­˜å ç”¨å¤§å¹…ä¸‹é™ï¼Œæ›´æ–°çš„å‚æ•°ä¹Ÿä»æ•°äº¿å‡å°‘åˆ°å‡ ç™¾ä¸‡ã€‚
####

-----------------------------------------------------------------------------------------------------------

Step 4 è®­ç»ƒ

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["validation"],
)
trainer.train()

----------------------------------------------------------------------------------------
# è®­ç»ƒå®Œæ¯•ä¹‹åä¿å­˜æƒé‡ã€åˆ†è¯å™¨åˆ°æœ¬åœ°ç›®å½•
trainer.save_model("./saved_model")
tokenizer.save_pretrained("./saved_model")


from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained("./saved_model")
tokenizer = AutoTokenizer.from_pretrained("./saved_model")













